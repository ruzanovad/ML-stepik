{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxIquH6GK7kt"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500 height=450/></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>\"Глубокое обучение\". Базовый поток</b></h3>\n",
    "\n",
    "<h2 style=\"text-align: center;\"><b>Домашнее задание. Производная, градиент и градиентный спуск\n",
    "</b></h2>\n",
    "\n",
    "В этом домашнем задании вам предстоит поработать с понятием производной и градиента, а также написать градиентный спуск и его вариации.\n",
    "\n",
    "__Напоминание:__\n",
    "Производной функции $f$ в точке $x$ называется выражение\n",
    "\n",
    "$$\\lim_{h→0}\\frac{f(x+h)−f(x)}{h}$$\n",
    "Или, что то же самое,\n",
    "$$\\lim_{x→x_0}\\frac{f(x)−f(x_0)}{x-x_0}$$\n",
    "\n",
    "Если такой предел существует, то и производная существует (и равна этому пределу)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MIzAFQsnvod2"
   },
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6JiQgamvod9"
   },
   "source": [
    "## Задание 1 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y38bKm9Tvod-"
   },
   "source": [
    "Какие из перечисленных функций имеют производную в нуле $(x_0 = 0)$?\n",
    "\n",
    "1) $f(x) = |x|^2$\n",
    "\n",
    "2) $f(x) = \\frac{sin(x)}{x}$\n",
    "\n",
    "3) $f(x) = |x|$\n",
    "\n",
    "4) $f(x) = \n",
    "     \\begin{cases}\n",
    "       x^2, &\\text{$x \\ne 0$}; \\\\\n",
    "       0, &\\text{$x = 0$}\n",
    "     \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfdVpqeTvoeA"
   },
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc4hJhHuvoeC"
   },
   "source": [
    "## Задание 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_MN9myzvoeD"
   },
   "source": [
    "Посчитайте производную $f(x)=x^x$ в точке $x_0 = e$\n",
    "\n",
    "Ответ округлите до одного знака после запятой.\n",
    "\n",
    "*Указание*. Представьте функцию $f(x)$ как $e^{g(x)}$ для некоторой $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBcFA6ErvoeF"
   },
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsJ_W3VXvoeH"
   },
   "source": [
    "## Задание 3 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj-eLSJyvoeI"
   },
   "source": [
    "Вычислите производную $f(x)=tg(x)⋅\\ln(\\cos(x^2)+1)$, в точке $x_0 = 0$. Ответ округлите до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctY73lF1voeK"
   },
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHjndqfCvoeL"
   },
   "source": [
    "## Задание 4 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve1wPT74voeN"
   },
   "source": [
    "​Ваше задание --- написать python-функцию, которая в качестве аргумента принимает:\n",
    "\n",
    "числовую функцию $f$, у которой необходимо вычислить производную\n",
    "число $\\varepsilon$ --- его необходимо использовать в качестве \"малого шага\" для приближённого вычисления производной.\n",
    "Функция должна в свою очередь возвращать числовую функцию $f'$, равную производной функции $f$.\n",
    "\n",
    "Однако не подумайте, что вас просят написать что-то, что будет вычислять эту самую производную аналитически. Производную следует вычислять по формуле $$f'(x)\\approx \\frac{f(x+\\varepsilon) - f(x)}{\\varepsilon}.$$\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tYuZF08nvoeO"
   },
   "outputs": [],
   "source": [
    "def numerical_derivative_1d(func, epsilon):\n",
    "    \"\"\"\n",
    "    Функция для приближённого вычисления производной функции одной переменной. \n",
    "    :param func: float -> float — произвольная дифференцируемая функция\n",
    "    :param epsilon: float — максимальная величина приращения по оси Ох\n",
    "    :return: другая функция, которая приближённо вычисляет производную в точке\n",
    "    \"\"\"\n",
    "    def deriv_func(x):\n",
    "        \"\"\"\n",
    "        :param x: float — точка, в которой нужно вычислить производную\n",
    "        :return: приближённое значение производной в этой точке\n",
    "        \"\"\"\n",
    "        return #YOUR CODE\n",
    "        \n",
    "    return deriv_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ogo1thL-voeT"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25016/2191016873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mestimation_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimed_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mapprox_deriv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimation_error\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdebug_print\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimation_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimed_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapprox_deriv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Проверьте себя!\n",
    "def polynom_to_prime(x):\n",
    "    return 20 * x**5 + x**3 - 5 * x**2 + 2 * x + 2.0\n",
    "\n",
    "\n",
    "def primed_poly(x):\n",
    "    return 100 * x**4 + 3 * x**2 -10 * x + 2.0\n",
    "\n",
    "\n",
    "approx_deriv = numerical_derivative_1d(polynom_to_prime, 1e-5)\n",
    "\n",
    "grid = np.linspace(-2, 2, 100)\n",
    "right_flag = True\n",
    "tol = 0.05\n",
    "debug_print = []\n",
    "\n",
    "for x in grid:\n",
    "    estimation_error = abs(primed_poly(x) - approx_deriv(x)) \n",
    "    if estimation_error > tol:\n",
    "        debug_print.append((estimation_error, primed_poly(x), approx_deriv(x)))\n",
    "        right_flag = False\n",
    "\n",
    "if not right_flag:\n",
    "    print(\"Что-то не то...\")\n",
    "    print(debug_print)\n",
    "    plt.plot(grid, primed_poly(grid), label=\"Истинная производная\")\n",
    "    plt.plot(grid, approx_deriv(grid), label=\"Численное приближение\")\n",
    "    plt.legend()\n",
    "\n",
    "print(str(right_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJVDE5FovoeZ"
   },
   "source": [
    "## Задание 5 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91UYo01wvoea"
   },
   "source": [
    "В этом задании Вы должны найти минимум функций с помощью градиентного спуска.\n",
    "\n",
    "Вам на вход подаются функция `func`, ее производная `deriv` (*), а также начальная точка `start`, на выходе - точка локального минимума. Для вашего удобства мы написали функцию для отрисовки траектории градиентного спуска\n",
    "\n",
    "(*) - вам не нужно будет ее вычислять. То, что вы написали в предыдущем задании, вам пригодится чуть позже.\n",
    "\n",
    "В первой реализации градиентного спуска можете предполагать, что на вход подаются функции с единственным, глобальным минимумом. Перед тем, как писать код, ответьте себе на следующие вопросы:\n",
    "\n",
    "* Как понять, что пора остановиться? Это может зависеть от градиента или расстояния между двумя соседними шагами алгоритма, так и от числа уже выполненных итераций.\n",
    "* Как правильно менять величину шага (`learning rate`) от итерации к итерации?\n",
    "\n",
    "В этом пункте гарантируется, что существует решение, использующее обычный градиентный спуск с фиксированным learning rate и наперёд заданным количеством итераций. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfBgQXzxfKRk"
   },
   "source": [
    "На каждой итерации вызывайте `callback(x, f(x))`, где `x` это результат шага градиентного спуска.   \n",
    "Это нужно для отрисовки шагов алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6o-LAqEHARF1"
   },
   "outputs": [],
   "source": [
    "def grad_descent_v1(f, deriv, x0=None, lr=0.1, iters=100, callback=None):\n",
    "    \"\"\"\n",
    "    Реализация градиентного спуска для функций с одним локальным минимумом,\n",
    "    совпадающим с глобальным. Все тесты будут иметь такую природу.\n",
    "    :param func: float -> float — функция\n",
    "    :param deriv: float -> float — её производная\n",
    "    :param x0: float — начальная точка\n",
    "    :param lr: float — learning rate\n",
    "    :param iters: int — количество итераций\n",
    "    :param callback: callable — функция логирования\n",
    "    \"\"\"\n",
    "\n",
    "    if x0 is None:\n",
    "        # Если точка не дана, сгенерируем случайную\n",
    "        # из равномерного распределения.\n",
    "        # При таком подходе начальная точка может быть\n",
    "        # любой, а не только из какого-то ограниченного диапазона\n",
    "        np.random.seed(179)\n",
    "        x0 = np.random.uniform()\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    callback(x, f(x)) # не забывайте логировать\n",
    "    i = 1\n",
    "    while i <= iters or abs(lr*deriv(x)) > lr:\n",
    "        x -= lr*deriv(x)\n",
    "        i+=1\n",
    "        callback(x, f(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-Is2v1J4dJy"
   },
   "source": [
    "### Отрисовка и тесты\n",
    "Рекомедуем пользоваться!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kwZyc1vVd3h9"
   },
   "outputs": [],
   "source": [
    "def plot_convergence_1d(func, x_steps, y_steps, ax, grid=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Функция отрисовки шагов градиентного спуска. \n",
    "    Не меняйте её код без необходимости! \n",
    "    :param func: функция, которая минимизируется градиентным спуском\n",
    "    :param x_steps: np.array(float) — шаги алгоритма по оси Ox\n",
    "    :param y_steps: np.array(float) — шаги алгоритма по оси Оу\n",
    "    :param ax: холст для отрисовки графика\n",
    "    :param grid: np.array(float) — точки отрисовки функции func\n",
    "    :param title: str — заголовок графика\n",
    "    \"\"\"\n",
    "    ax.set_title(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    if grid is None:\n",
    "        grid = np.linspace(np.min(x_steps), np.max(x_steps), 100)\n",
    "\n",
    "    fgrid = [func(item) for item in grid]\n",
    "    ax.plot(grid, fgrid)\n",
    "    yrange = np.max(fgrid) - np.min(fgrid)\n",
    "\n",
    "    arrow_kwargs = dict(linestyle=\"--\", color=\"grey\", alpha=0.4)\n",
    "    for i, _ in enumerate(x_steps):\n",
    "        if i + 1 < len(x_steps):\n",
    "            ax.arrow(\n",
    "                x_steps[i], y_steps[i], \n",
    "                x_steps[i + 1] - x_steps[i],\n",
    "                y_steps[i + 1] - y_steps[i], \n",
    "                **arrow_kwargs\n",
    "            )\n",
    "\n",
    "    n = len(x_steps)\n",
    "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
    "    ax.scatter(x_steps, y_steps, c=color_list)\n",
    "    ax.scatter(x_steps[-1], y_steps[-1], c=\"red\")\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$y$\")\n",
    "\n",
    "\n",
    "class LoggingCallback:\n",
    "    \"\"\"\n",
    "    Класс для логирования шагов градиентного спуска. \n",
    "    Сохраняет точку (x, f(x)) на каждом шаге.\n",
    "    Пример использования в коде: callback(x, f(x))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.x_steps = []\n",
    "        self.y_steps = []\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        self.x_steps.append(x)\n",
    "        self.y_steps.append(y)\n",
    "\n",
    "\n",
    "def test_convergence_1d(grad_descent, test_cases, tol=1e-2, axes=None, grid=None):\n",
    "    \"\"\"\n",
    "    Функция для проверки корректности вашего решения в одномерном случае.\n",
    "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
    "    :param grad_descent: ваша реализация градиентного спуска\n",
    "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
    "        - \"func\" — функция (обязательно)\n",
    "        - \"deriv\" — её производная (обязательно)\n",
    "        - \"start\" — начальная точка start (м.б. None) (опционально) \n",
    "        - \"low\", \"high\" — диапазон для выбора начальной точки (опционально)\n",
    "        - \"answer\" — ответ (обязательно)\n",
    "    При желании вы можете придумать и свои тесты.\n",
    "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
    "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
    "    :param grid: np.array(float), точки на оси Ох для отрисовки тестов\n",
    "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
    "    \"\"\"\n",
    "    right_flag = True\n",
    "    debug_log = []\n",
    "    for i, key in enumerate(test_cases.keys()):\n",
    "        # Формируем входные данные и ответ для алгоритма.\n",
    "        answer = test_cases[key][\"answer\"]\n",
    "        test_input = deepcopy(test_cases[key])\n",
    "        del test_input[\"answer\"]\n",
    "        # Запускаем сам алгоритм.\n",
    "        callback = LoggingCallback()  # Не забываем про логирование\n",
    "        res_point = grad_descent(*test_input.values(), callback=callback)\n",
    "        # Отрисовываем результаты.\n",
    "        if axes is not None:\n",
    "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
    "            x_steps = np.array(callback.x_steps)\n",
    "            y_steps = np.array(callback.y_steps)\n",
    "            plot_convergence_1d(\n",
    "                test_input[\"func\"], x_steps, y_steps, \n",
    "                ax, grid, key\n",
    "            )\n",
    "            ax.axvline(answer, 0, linestyle=\"--\", c=\"red\",\n",
    "                        label=f\"true answer = {answer}\")\n",
    "            ax.axvline(res_point, 0, linestyle=\"--\", c=\"xkcd:tangerine\", \n",
    "                        label=f\"estimate = {np.round(res_point, 3)}\")\n",
    "            ax.legend(fontsize=16)\n",
    "        # Проверяем, что найдення точка достаточно близко к истинной\n",
    "        if abs(answer - res_point) > tol or np.isnan(res_point):\n",
    "            debug_log.append(\n",
    "                f\"Тест '{key}':\\n\"\n",
    "                f\"\\t- ответ: {answer}\\n\"\n",
    "                f\"\\t- вывод алгоритма: {res_point}\"\n",
    "            )\n",
    "            right_flag = False\n",
    "    return right_flag, debug_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1eOO6t0cvoeh"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_descent_v1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8372/1761622828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m is_correct, debug_log = test_convergence_1d(\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mgrad_descent_v1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_descent_v1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_cases = {\n",
    "    \"square\": {\n",
    "        \"func\" : lambda x: x * x, \n",
    "        \"deriv\" : lambda x: 2 * x, \n",
    "        \"start\" : 2, \n",
    "        \"answer\" : 0.0\n",
    "    },\n",
    "    \"module\": {\n",
    "        \"func\" : lambda x: abs(x),  \n",
    "        \"deriv\" : lambda x: 1 if x > 0 else -1,\n",
    "        \"start\" : 2, \n",
    "        \"answer\" : 0.0\n",
    "    },\n",
    "    \"third_power\": {\n",
    "        \"func\" : lambda x: abs((x - 1)**3),\n",
    "        \"deriv\" : lambda x: 3 * (x - 1)**2 * np.sign(x-1),\n",
    "        \"start\" : -1, \n",
    "        \"answer\" : 1.0\n",
    "    },\n",
    "    \"ln_x2_1\": {\n",
    "        \"func\" : lambda x: np.log((x + 1)**2 + 1),  \n",
    "        \"deriv\" : lambda x: 2 * (x + 1) / (x**2 +1), \n",
    "        \"start\" : 1, \n",
    "        \"answer\" : -1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tol = 1e-2  # желаемая точность \n",
    "fig, axes = plt.subplots(2,2, figsize=(14, 12))\n",
    "fig.suptitle(\"Градиентный спуск, версия 1\", fontweight=\"bold\", fontsize=20)\n",
    "grid = np.linspace(-2, 2, 100)\n",
    "\n",
    "is_correct, debug_log = test_convergence_1d(\n",
    "    grad_descent_v1, test_cases, tol, \n",
    "    axes, grid\n",
    ")\n",
    "if not is_correct:\n",
    "    print(\"Не сошлось. Дебажный вывод:\")\n",
    "    for log_entry in debug_log:\n",
    "        print(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjYYcxG0voel"
   },
   "source": [
    "## Задание 6 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bqka1-1voen"
   },
   "source": [
    "Это задание чуть сложнее. Если раньше Вам нужно было просто найти минимум у довольно хорошей функции, то сейчас в тестах будут плохие. У них может быть несколько локальных минимумов, вам же нужно найти глобальный минимум у каждой функции.\n",
    "\n",
    "В общем случае такая задача невыполнима, но у вас будут одномерные функции и все самое интересное будет сосредоточено в районе нуля. А именно, известно что глобальный минимум лежит в пределах (`low`, `high`) (параметры алгоритма). Вам нужно модифицировать градиентный спуск, который вы написали в предыдущем задании, чтобы он работал и в таком случае. \n",
    "\n",
    "Сначала запустите градиентный спуск из прошлого пункта на тестах из ноутбука. Скорее всего, некоторые из них не пройдут. Подумайте, как исправить ситуацию.\n",
    "\n",
    "И снова не забывайте вызывать `callback(x, f(x))` на каждом шаге алгоритма!\n",
    "\n",
    "**Возможное решение** Если вы хотите поэкспериментировать и ощутить всю боль от оптимизации таких функций, сначала подумайте сами, не пытаясь следовать нашим указаниям. Тем не менее, для тех из вас, у кого таких наклонностей нет, мы выписали одно из возможных решений, которое приводит к успеху.\n",
    "\n",
    "\n",
    "\n",
    "* Сделайте шаг обучения не константным, а зависящим от номера итерации. Неплохая эвристика --- домножать `lr` на $ \\frac{1}{ \\sqrt{iteration}}$. \n",
    "\n",
    "* В этой задаче в функциях могут после первого же шага градиентного спуска появляться очень большие значения. Для того, чтобы не вылезать за пределы отрезка, на котором ищется минимум, после каждого шага спуска используйте ``np.clip`` к очередной точке ``x_n``. \n",
    "\n",
    "* Разбейте весь отрезок на несколько (3-6) подотрезков и найдите минимум на каждом из отрезков (на каждом отрезке, кстати, можно сделать больше одного запуска). Затем из всех найденных результатов выберите минимальный. \n",
    "\n",
    "* Авторское решение использует параметры ``iters = 5000`` и ``lr = 0.05``\n",
    " \n",
    "\n",
    "Больше о тонкостях градиентного спуска можно прочитать, например, в <a href=https://github.com/amkatrutsa/optimization-fivt/blob/master/07-GD/lecture7.pdf>лекциях МФТИ</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OsdOjX743Hoj"
   },
   "outputs": [],
   "source": [
    "def grad_descent_v2(f, df, low=None, high=None, callback=None):\n",
    "    \"\"\" \n",
    "    Реализация градиентного спуска для функций с несколькими локальным минимумами,\n",
    "    но с известной окрестностью глобального минимума. \n",
    "    Все тесты будут иметь такую природу.\n",
    "    :param func: float -> float — функция \n",
    "    :param deriv: float -> float — её производная\n",
    "    :param low: float — левая граница окрестности\n",
    "    :param high: float — правая граница окрестности\n",
    "    :param callback: callalbe -- функция логирования\n",
    "    \"\"\"\n",
    "    def find_local_min(f, df, low_local, high_local, iters=5000, lr=0.05):\n",
    "        #функция для нахождения минимума функции f на промежутке (low_local, high_local)\n",
    "        x0 = np.random.uniform(low_local, high_local)\n",
    "        x = x0\n",
    "        \n",
    "        for i in range(1, iters+1):\n",
    "            lr /= np.sqrt(i)\n",
    "            x -= lr * df(x) \n",
    "            x = np.clip(np.array([x]), low_local, high_local)[0]\n",
    "            callback(x, f(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    parts = 3\n",
    "    \n",
    "    list_of_borders = [[low + i*(high-low)/parts, low + (i+1)*(high-low)/parts] for i in range(parts)]\n",
    "    list_of_x = []\n",
    "\n",
    "    for i, j in list_of_borders:\n",
    "        for k in range(10):\n",
    "            list_of_x.append(find_local_min(f, df, i, j))\n",
    "    list_of_y = [f(x) for x in list_of_x]\n",
    "        \n",
    "    best_estimate = list_of_x[np.argmin(list_of_y)]\n",
    "    \n",
    "    return best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgXx51m4voeu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\light\\AppData\\Local\\Temp/ipykernel_8372/179562656.py:23: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \"func\" : lambda x: 5 * np.abs(x)/x**2 - 0.5 * x + 0.1 * np.sqrt(-x) + 0.01 * x**2 ,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не сошлось. Дебажный вывод:\n",
      "Тест 'poly1':\n",
      "\t- ответ: -1.88\n",
      "\t- вывод алгоритма: -1.869965435156833\n",
      "Тест 'and another yet poly':\n",
      "\t- ответ: 0.994502\n",
      "\t- вывод алгоритма: 0.9681516512810037\n",
      "Тест '|x|/x^2 - x + sqrt(-x) + (even polynom)':\n",
      "\t- ответ: -2.91701\n",
      "\t- вывод алгоритма: -2.9313185652306566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\light\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    }
   ],
   "source": [
    "test_cases = {\n",
    "    \"poly1\" : {\n",
    "        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 1.5 * x + 1,\n",
    "        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 1.5,\n",
    "        \"low\" : -4, \"high\" : 2, \"answer\" : -1.88\n",
    "    },\n",
    "    \"poly2\" : {\n",
    "        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 2 * x + 1.0,\n",
    "        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 2.0, \n",
    "        \"low\" : -3, \"high\" : 3, \"answer\" : 0.352\n",
    "    },\n",
    "    \"another yet poly\" : {\n",
    "        \"func\" : lambda x: x**6 + x**4 - 10 * x**2 - x ,\n",
    "        \"deriv\" : lambda x: 6 * x**5 + 4 * x**3 - 20 * x - 1, \n",
    "        \"low\" : -2, \"high\" : 2, \"answer\" : 1.24829\n",
    "    },\n",
    "    \"and another yet poly\" : {\n",
    "        \"func\" : lambda x: x**20 + x**2 - 20 * x + 10,\n",
    "        \"deriv\" : lambda x: 20 * x**19 + 2 * x - 20, \n",
    "        \"low\" : -0, \"high\" : 2, \"answer\" : 0.994502\n",
    "    },\n",
    "    \"|x|/x^2 - x + sqrt(-x) + (even polynom)\" : {\n",
    "        \"func\" : lambda x: 5 * np.abs(x)/x**2 - 0.5 * x + 0.1 * np.sqrt(-x) + 0.01 * x**2 ,\n",
    "        \"deriv\" : lambda x: -0.5 - 0.05/np.sqrt(-x) + 0.02 * x + 5/(x * np.abs(x)) - (10 * np.abs(x))/x**3,\n",
    "        \"low\" : -4, \"high\" : -2, \"answer\" : -2.91701\n",
    "    },\n",
    "}\n",
    "\n",
    "tol = 1e-2 # желаемая точность\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(24, 8))\n",
    "fig.suptitle(\"Градиентный спуск, версия 2\", fontweight=\"bold\", fontsize=20)\n",
    "grid = np.linspace(-3, 3, 100)\n",
    "\n",
    "is_correct, debug_log = test_convergence_1d(\n",
    "    grad_descent_v2, test_cases, tol, \n",
    "    axes, grid\n",
    ")\n",
    "\n",
    "if not is_correct:\n",
    "    print(\"Не сошлось. Дебажный вывод:\")\n",
    "    for log_entry in debug_log:\n",
    "        print(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pl7exWe3YLd"
   },
   "source": [
    "​\n",
    "# Основные положения дифференциального исчисления функций многих переменных\n",
    "\n",
    "Если вдруг Вы не знаете или не помните дифференциальное исчисление функций многих переменных, то вот несколько ключевых определений, которые помогут Вам справиться с заданиями 7-9. Здесь и ниже рассматриваются скалярные функции многих переменных, т.е. $f:U\\rightarrow \\mathbb{R}$, где $U$ --- область в $\\mathbb{R}^n$. \n",
    "\n",
    "\n",
    "Мы хотим ввести некоторый аналог производной. Что мы можем делать уже сейчас --- это вычислять производные функции многих переменных по отдельным аргументам.\n",
    "\n",
    "## Дифференцируемость функции многих переменных\n",
    "**Определение**.* Частной производной функции нескольких переменных* $f(x_1,x_2,\\ldots, x_n)$ по аргументу $x_i$ в точке $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$ называется производная функции $f$ по $x_i$ в точке $x^0_i$ как функции одного аргумента при фиксированных значениях $x_1^0$, $x_2^0, \\ldots$, $x_{i-1}^0$, $x_{i+1}^0, \\ldots$, $x_n^0$. Иными словами, частная производная равна вот такому пределу:\n",
    "\n",
    "$$\\lim_{h \\to 0}\\frac{f(x_1,\\ldots,x_{i-1}, x_i + h, x_{i+1}, \\ldots, x_n) - f(x_1,\\ldots,x_n)}{h}.$$\n",
    "\n",
    "**Обозначение**:\n",
    "\n",
    "$$f'_{x_i}(\\overline{x^0}); \\quad \\frac{\\partial f}{\\partial x_i}(\\overline{x_0}).$$\n",
    "\n",
    "**Определение**. Функция $f: \\mathbb{R}^n \\to \\mathbb{R}$ называется *дифференцируемой в точке* $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$, если имеет место представление\n",
    "$$f(\\overline{x^0} + \\overline{\\Delta x}) = f(\\overline{x^0}) + \\langle \\overline{a}, \\overline{\\Delta x} \\rangle + o(|\\overline{\\Delta x}|),$$ где $\\overline{a} \\in \\mathbb{R}^n$ --- некоторый $n$-мерный вектор, который называется градиентом функции $f$ в точке $\\overline{x^0}$. \n",
    "\n",
    "**Обозначения для градиента**:\n",
    "$$\\mathrm{grad} f(\\overline{x^0}) = \\nabla f(\\overline{x^0}) = f'(\\overline{x^0}).$$\n",
    "\n",
    "## Связь градиента и частных производных\n",
    "Как можно заметить, определение выше, во-первых, полностью аналогично свойству производной функции одной переменной. Во-вторых, это определение довольно бесполезно. Оказывается, во всех \"хороших\" случаях справедливо следующее утверждение.\n",
    "\n",
    "**Теорема**.\n",
    "Пусть функция $f$ имеет в точке $\\overline{x^0}$ непрерывные частные производные по каждой компоненте $x_i$. Тогда $f$ дифференцируема в точке $\\overline{x^0}$, причём её градиент равен вектору из частных производных, то есть \n",
    "$$\\left(\\frac{\\partial f}{\\partial x_1}(\\overline{x^0}), \\frac{\\partial f}{\\partial x_2}(\\overline{x^0}), \\ldots, \\frac{\\partial f}{\\partial x_n}(\\overline{x^0})\\right).$$\n",
    "\n",
    "Суть этой теоремы заключается в том, что во всех ``хороших'' случаях градиент существует и его очень просто вычислить --- нужно просто посчитать частные производные по всем переменным.\n",
    "\n",
    "**Замечание**.\n",
    "Градиент указывает на направление наискорейшего роста значения функции. Иными словами, при движении точки, стартующей в $\\overline{x^0}$, по вектору $\\mathrm{grad} f(\\overline{x^0})$, значение функции увеличивается.\n",
    "\n",
    " \n",
    "\n",
    "## Алгоритм градиентного спуска\n",
    "Алгоритм градиентного спуска для поиска минимума функции $n$ переменных $f(x_1,x_2,\\ldots, x_n)$ состоит в итеративном поиске точки минимума функции по формуле для $k+1$-ой точки через $k$-ую точку и градиент в $k$-ой точке.\n",
    "\n",
    "$$\\overline{x}^{k+1} = \\overline{x}^k - \\lambda\\nabla f(\\overline{x}^k),$$\n",
    "\n",
    "где $\\lambda$ --- положительное число, называемое learning rate. Обратите внимание, что здесь верхние индексы обозначают не степень, а номер точки в последовательности! То есть под $\\overline{x}^k$ понимается вектор $(x^k_1,\\ldots, x^k_n)$ --- $k$-ая точка из последовательности приближений, которую строит алгоритм.\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxQWFDuovoe-"
   },
   "source": [
    "## Задание 7 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-8n8ZzIvoe_"
   },
   "source": [
    "В лекции было несколько функций, чьи градиенты Вам было предложено вычислить.\n",
    "\n",
    "Вычислите градиент следующей функции:\n",
    "\n",
    "$$\\psi(x,y,z) = sin(xz) - y^2z + e^x$$\n",
    "\n",
    "Заполните пропуски в коде"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q47B6rkvofB"
   },
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4uFHoIfsV_0"
   },
   "outputs": [],
   "source": [
    "from math import sin, cos, tan, exp, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def grad_1(x, y, z):\n",
    "    #возвращает кортеж из 3 чисел --- частных производных по x,y,z \n",
    "    \n",
    "    dx = z * cos(z * x) + exp(x) \n",
    "    dy = - z * 2 * y\n",
    "    dz = x * cos (z * x) - y * y\n",
    "    return (dx, dy, dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKK81eg_ikMr"
   },
   "outputs": [],
   "source": [
    "#Тестируем нашу функцию\n",
    "import numpy as np\n",
    "\n",
    "assert np.allclose(grad_1(1,1,1), (3.258584134327185, -2, -0.45969769413186023), atol=5e-6)\n",
    "assert np.allclose(grad_1(1, 8, 0), (2.718281828459045, 0, -63.0), atol=5e-6)\n",
    "assert np.allclose(grad_1(-11,pi,1), (0.004442399688841031, -6.283185307179586, -9.918287078957917), atol=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ3JJG_mvofD"
   },
   "source": [
    "## Задание 8 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn9o18L5vofE"
   },
   "source": [
    "Еще один градиент, похожий на тот, что был на лекции:\n",
    "\n",
    "$\\psi(x,y,z) = ln(cos(e^{x+y})) - ln(xy)$ \n",
    "\n",
    "Заполните пропуски в функции ниже\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYcDdBZ2-BaT"
   },
   "outputs": [],
   "source": [
    "def grad_2(x, y, z):\n",
    "    #возвращает кортеж из 3 чисел --- частных производных по x,y,z \n",
    "    dx = #YOUR CODE\n",
    "    dy = #YOUR CODE\n",
    "    dz = #YOUR CODE\n",
    "    return (dx, dy, dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAjyySeKi8YN"
   },
   "outputs": [],
   "source": [
    "#Тестируем нашу функцию\n",
    "\n",
    "assert np.allclose(grad_2(1,1,0), (-15.73101919885423, -15.73101919885423, 0), atol=5e-6)\n",
    "assert np.allclose(grad_2(-10, 3, 0), (0.09999916847105042, -0.3333341648622829, 0), atol=5e-6)\n",
    "assert np.allclose(grad_2(15 ,4, 0), (54654806.79650013, 54654806.6131668,0), atol=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJi66t39voey"
   },
   "source": [
    "## Задание 9 ##\n",
    "А теперь все вместе!\n",
    "\n",
    "У вас есть только функция, которую Вам отдают в качестве аргумента и вы должны найти её минимум.\n",
    "\n",
    "Вы будете искать глобальный, у вас это должно получиться лишь потому, что тут они хорошие.\n",
    "\n",
    "Да, и еще, теперь они не одномерные, а двумерные. Также вам будут даны начальные точки, сходимость из которых гарантируется.\n",
    "\n",
    "***Подсказка*** можете использовать следующие параметры:\n",
    "\n",
    "* Отклонение при вычислении производной $\\varepsilon = 10^{-10}$\n",
    "* Критерий остановки: кол-во итераций $10^4$\n",
    "* Длина шага градиентного спуска $lr = 0.5$\n",
    "\n",
    "\n",
    "И вновь мы предоставляем функцию отрисовки шагов для пущего удобства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egm2nGHCvoe1"
   },
   "outputs": [],
   "source": [
    "def numerical_derivative_2d(func, epsilon):\n",
    "    \"\"\"\n",
    "    Функция для приближённого вычисления градиента функции двух переменных. \n",
    "    :param func: np.array[2] -> float — произвольная дифференцируемая функция\n",
    "    :param epsilon: float — максимальная величина приращения по осям\n",
    "    :return: другая функция, которая приближённо вычисляет градиент в точке\n",
    "    \"\"\"\n",
    "    def grad_func(x):\n",
    "        \"\"\"\n",
    "        :param x: np.array[2] — точка, в которой нужно вычислить градиент\n",
    "        :return: np.array[2] — приближённое значение градиента в этой точке\n",
    "        \"\"\"\n",
    "        dx = (func(np.array([x[0] + epsilon, x[1]])) - func(x)) / epsilon\n",
    "        dy = (func(np.array([x[0], x[1] + epsilon])) - func(x)) / epsilon\n",
    "        return np.array([dx, dy])\n",
    "\n",
    "    return grad_func\n",
    "\n",
    "\n",
    "def grad_descent_2d(func, low, high, start=None, callback=None):\n",
    "    \"\"\" \n",
    "    Реализация градиентного спуска для функций двух переменных \n",
    "\n",
    "    Обратите внимание, что здесь градиент функции не дан.\n",
    "    Его нужно вычислять приближённо.\n",
    "\n",
    "    :param func: np.ndarray -> float — функция \n",
    "    :param low: левая граница интервала по каждой из осей\n",
    "    :param high: правая граница интервала по каждой из осей\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    df = numerical_derivative_2d(func, eps)\n",
    "    if start is None:\n",
    "        start = np.array([np.random.uniform(low, high), np.random.uniform(low, high)])\n",
    "    x = start\n",
    "    \n",
    "    lr = 0.5\n",
    "    for i in range(1, 10000+1):\n",
    "        x[0] -= lr * df(x)[0]\n",
    "        x[1] -= lr * df(x)[1]\n",
    "        callback(x, func(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARnTlYWcLGo0"
   },
   "outputs": [],
   "source": [
    "def plot_convergence_2d(func, steps, ax, xlim, ylim, cmap=\"viridis\", title=\"\"):\n",
    "    \"\"\"\n",
    "    Функция отрисовки шагов градиентного спуска. \n",
    "    Не меняйте её код без необходимости! \n",
    "    :param func: функция, которая минимизируется градиентным спуском\n",
    "    :param steps: np.array[N x 2] — шаги алгоритма\n",
    "    :param ax: холст для отрисовки графика\n",
    "    :param xlim: tuple(float), 2 — диапазон по первой оси\n",
    "    :param ylim: tuple(float), 2 — диапазон по второй оси\n",
    "    :param cmap: str — название палитры\n",
    "    :param title: str — заголовок графика\n",
    "    \"\"\"\n",
    "\n",
    "    ax.set_title(title, fontsize=20, fontweight=\"bold\")\n",
    "    # Отрисовка значений функции на фоне\n",
    "    xrange = np.linspace(*xlim, 100)\n",
    "    yrange = np.linspace(*ylim, 100)\n",
    "    grid = np.meshgrid(xrange, yrange)\n",
    "    X, Y = grid\n",
    "    fvalues = func(\n",
    "        np.dstack(grid).reshape(-1, 2)\n",
    "    ).reshape((xrange.size, yrange.size))\n",
    "    ax.pcolormesh(xrange, yrange, fvalues, cmap=cmap, alpha=0.8)\n",
    "    CS = ax.contour(xrange, yrange, fvalues)\n",
    "    ax.clabel(CS, CS.levels, inline=True)\n",
    "    # Отрисовка шагов алгоритма в виде стрелочек\n",
    "    arrow_kwargs = dict(linestyle=\"--\", color=\"black\", alpha=0.8)\n",
    "    for i, _ in enumerate(steps):\n",
    "        if i + 1 < len(steps):\n",
    "            ax.arrow(\n",
    "                *steps[i],\n",
    "                *(steps[i+1] - steps[i]),\n",
    "                **arrow_kwargs\n",
    "            )\n",
    "    # Отрисовка шагов алгоритма в виде точек\n",
    "    n = len(steps)\n",
    "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
    "    ax.scatter(steps[:, 0], steps[:, 1], c=color_list, zorder=10)\n",
    "    ax.scatter(steps[-1, 0], steps[-1, 1], \n",
    "               color=\"red\", label=f\"estimate = {np.round(steps[-1], 2)}\")\n",
    "    # Финальное оформление графиков\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFgjqiYZvoe5"
   },
   "outputs": [],
   "source": [
    "def test_convergence_2d(grad_descent_2d, test_cases, tol, axes=None):\n",
    "    \"\"\"\n",
    "    Функция для проверки корректности вашего решения в двумерном случае.\n",
    "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
    "    :param grad_descent_2d: ваша реализация градиентного спуска\n",
    "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
    "        - \"func\" — функция \n",
    "        - \"deriv\" — её производная \n",
    "        - \"low\", \"high\" — диапазон для выбора начальной точки \n",
    "        - \"answer\" — ответ \n",
    "    При желании вы можете придумать и свои тесты.\n",
    "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
    "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
    "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
    "    \"\"\"\n",
    "    right_flag = True\n",
    "    debug_log = []\n",
    "    for i, key in enumerate(test_cases.keys()):\n",
    "        # Формируем входные данные и ответ для алгоритма.\n",
    "        answer = test_cases[key][\"answer\"]\n",
    "        test_input = deepcopy(test_cases[key])\n",
    "        del test_input[\"answer\"]\n",
    "        # Запускаем сам алгоритм.\n",
    "        callback = LoggingCallback()  # Не забываем про логирование\n",
    "        res_point = grad_descent_2d(**test_input, callback=callback)\n",
    "        # Отрисовываем результаты.\n",
    "        if axes is not None:\n",
    "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
    "            plot_convergence_2d(\n",
    "                np.vectorize(test_input[\"func\"], signature=\"(n)->()\"), \n",
    "                np.vstack(callback.x_steps), \n",
    "                ax=ax, \n",
    "                xlim=(test_input[\"low\"], test_input[\"high\"]), \n",
    "                ylim=(test_input[\"low\"], test_input[\"high\"]),\n",
    "                title=key\n",
    "            )   \n",
    "        # Проверяем, что найденная точка достаточно близко к истинной\n",
    "        if np.linalg.norm(answer - res_point, ord=1) > tol:\n",
    "            debug_log.append(\n",
    "                f\"Тест '{key}':\\n\"\n",
    "                f\"\\t- ответ: {answer}\\n\"\n",
    "                f\"\\t- вывод алгоритма: {res_point}\"\n",
    "            )\n",
    "            right_flag = False\n",
    "    return right_flag, debug_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "vR_-Ucs4giUQ",
    "outputId": "5d3517c1-cce3-49b3-8499-e8fe5a299388"
   },
   "outputs": [],
   "source": [
    "test_cases = {\n",
    "    \"concentric_circles\" : {\n",
    "        \"func\" : lambda x: (\n",
    "            -1 / ((x[0] - 1)**2 + (x[1] - 1.5)**2 + 1)\n",
    "            * np.cos(2 * (x[0] - 1)**2 + 2 * (x[1] - 1.5)**2)\n",
    "        ),\n",
    "        \"low\" : -5,\n",
    "        \"high\" : 5,\n",
    "        \"start\": np.array([.2 , .7]),\n",
    "        \"answer\" : np.array([1, 1.5])\n",
    "    },\n",
    "        \"other concentric circles\" : {\n",
    "       \"func\" : lambda x: (\n",
    "            -1 / ((x[0])**2 + (x[1] - 3)**2 + 1)\n",
    "            * np.cos(2 * (x[0])**2 + 2 * (x[1] - 3)**2)\n",
    "        ),\n",
    "        \"low\" : -5,\n",
    "        \"high\" : 5,\n",
    "        \"start\": np.array([1.1, 3.3]),\n",
    "        \"answer\" : np.array([0, 3])\n",
    "    },\n",
    "    \"straightened ellipses\" : {\n",
    "        \"func\" : lambda x: (\n",
    "            -1 / ((x[0] )**4 + (x[1] - 3)**6 + 1)\n",
    "            * np.cos(2 * (x[0])**4 + 2 * (x[1] - 3)**6)\n",
    "        ),\n",
    "        \"low\" : -5,\n",
    "        \"high\" : 5,\n",
    "        \"start\": np.array([.8, 3.001]), # точка так близко к ответу тк в окрестности ответа градиент маленкьий и функция очень плохо сходится\n",
    "        \"answer\" : np.array([0, 3])\n",
    "    },\n",
    "}\n",
    "tol = 1e-2  # желаемая точность\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10), squeeze=False)\n",
    "fig.suptitle(\"Градиентный спуск в 2D\", fontsize=25, fontweight=\"bold\")\n",
    "is_correct, debug_log = test_convergence_2d(grad_descent_2d, test_cases, tol, axes)\n",
    "\n",
    "if not is_correct:\n",
    "    print(\"Не сошлось. Дебажный вывод:\")\n",
    "    for log_entry in debug_log:\n",
    "        print(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C0vjkL1P0dP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]gradient.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
